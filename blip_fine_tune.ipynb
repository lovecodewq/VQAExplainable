{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a271c6cf-07e6-4584-82df-d762cbc8eb86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Project - VQAExplanable Pt2\n",
    "**Author: Minyue Jin**\n",
    "\n",
    "_Inspired by HuggingFace [tutorial](https://huggingface.co/docs/transformers/main/tasks/visual_question_answering) on VQA_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5f7d4-65f0-463d-81fc-0a3055fc7bb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff73cf-dff7-4c97-98b5-9f7dc8f2784d",
   "metadata": {},
   "source": [
    "The top-down attention mechanism introduced in [this paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf) aims to efficiently fuse image features and text features. Another recent and widely adopted technique, Vision-Language Pretraining (VLP) also solves this issue in a different way, by pretraining the image and text encoders on a huge number of image-text pairs, and it's received tremendous success where it advances the performance for many multimodal downstream tasks, including VQA.\n",
    "\n",
    "In this section, I aim to answer the following questions to compare these two:\n",
    "1. Which one can better fuse image and text features?\n",
    "2. Can we combine them to have even better feature fusion?\n",
    "\n",
    "To do that, I plan to run the following experiments on a [VQA dataset](https://huggingface.co/datasets/HuggingFaceM4/VQAv2), and compare their performance:\n",
    "1. Finetune a pretrained [BLIP](https://arxiv.org/pdf/2201.12086) model\n",
    "2. Use the same pretrained BLIP model, add top-down attention, then finetune it\n",
    "3. Use ImageNet pretrained ViT (the same image encoder used in BLIP) + BERT (base text encoder used in BLIP) + BLIP decoder, and finetune it\n",
    "4. Use ImageNet pretrained ViT + BERT + BLIP decoder, add top-down attention, then finetune it\n",
    "\n",
    "Comparing 1 & 2 will help answer question 2; comparing 1 & 3 and 3 & 4 will help answer question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f466c2fd-e1df-4706-99d0-f37de68b57c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf532ef-2567-4225-af8f-c4578ba9cadf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fa5d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from datasets import load_dataset\n",
    "import aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb539dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, BlipForQuestionAnswering\n",
    "from transformers import DefaultDataCollator, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3352e025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ce7a14-14c3-4642-b75e-ff351c7d2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.blip.configuration_blip import BlipTextConfig\n",
    "from transformers.models.blip.modeling_blip import BlipTextVisionModelOutput\n",
    "from transformers.models.blip.modeling_blip_text import BlipTextLMHeadModel\n",
    "from typing import Any, Optional, Tuple, Union, Mapping, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b523faa-c126-4679-a136-f343e3e554c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig, AutoConfig, AutoModel, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e202f1-b26a-4d90-9919-6c09165b9b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a2b7d6-b35f-4839-a93d-8f12b4d1620a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fba8ae-7d91-49b1-a71b-925069640516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7edb5d-d077-4589-beaa-7a2d6dbd0f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38eb34b4-745f-44ee-81e6-de21c2f17ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b96994d-fbec-47da-803e-ae0b6796a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, FinerCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import FinerWeightedTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73e7b-9393-4887-9afa-0121a2496b4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup datasets\n",
    "\n",
    "In order to speed up finetuning, I'm only going to use 10% of the training set, and 1% of the validation set. Since the learning rate will be small, and the training will be short, the risk of overfitting is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ee8123c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "train_dset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\", split=\"train[:10%]\",\n",
    "    # avoid FSTimeOut, see https://github.com/huggingface/datasets/issues/7175\n",
    "    storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b55062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44376"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46323980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "val_dset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\", \n",
    "    split=\"validation[:1%]\",\n",
    "    storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1c437fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2144"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a5e2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WrappedVQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Wrapper around HF VQAv2 dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.processor(\n",
    "            self.dataset[idx]['image'], \n",
    "            self.dataset[idx]['question'], \n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            self.dataset[idx]['multiple_choice_answer'], \n",
    "            max_length=8, pad_to_max_length=True, truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        # remove batch dimension\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a21a7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf2dcb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"Salesforce/blip-vqa-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8bc3fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba17cdc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_wrapped = WrappedVQADataset(train_dset, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "257f693c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_wrapped = WrappedVQADataset(val_dset, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0e893-7399-4ad2-a0cd-36def063b86d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custom implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4178c65-2736-4784-a633-00e5fca0b138",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Top-down attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "573e435f-c194-41e4-88cf-fdcbb2b5a32a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TopDownAttention(nn.Module):\n",
    "    def __init__(self, image_feature_dim=768, question_dim=768, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(image_feature_dim + question_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, img_feats, q_feats):\n",
    "        padding_size = img_feats.shape[1] - q_feats.shape[1]\n",
    "        # Pad q_features on the second dimension so it can be concatenated with image_feats\n",
    "        q_feats_padded = F.pad(q_feats, (0, 0, 0, padding_size, 0, 0), \"constant\", 0)\n",
    "        joint_feats = torch.concat([img_feats, q_feats_padded], dim=-1)\n",
    "        joint_repr = torch.tanh(joint_feats)\n",
    "        attn_weights = self.attention_layer(joint_repr)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "        # Weighted sum of image features\n",
    "        weighted_img_feats = (attn_weights * img_feats).sum(dim=1)\n",
    "        return weighted_img_feats.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2003715d-9db6-4eb9-aa96-61a2e168e2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomBlipForQuestionAnswering(BlipForQuestionAnswering):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.top_down_attn = TopDownAttention()\n",
    "        self.post_init()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "    ) -> Union[Tuple, BlipTextVisionModelOutput]:\n",
    "        \"\"\"\n",
    "        Modified from https://github.com/huggingface/transformers/blob/main/src/transformers/models/blip/modeling_blip.py\n",
    "        \"\"\"\n",
    "        ### START COPY-PASTE ###\n",
    "        if labels is None and decoder_input_ids is None:\n",
    "            raise ValueError(\n",
    "                \"Either `decoder_input_ids` or `labels` should be passed when calling `forward` with\"\n",
    "                \" `BlipForQuestionAnswering`. if you are training the model make sure that `labels` is passed, if you\"\n",
    "                \" are using the model for inference make sure that `decoder_input_ids` is passed or call `generate`\"\n",
    "            )\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "        )\n",
    "\n",
    "        image_embeds = vision_outputs[0]\n",
    "        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
    "\n",
    "        question_embeds = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None:\n",
    "            # labels are already shifted right, see: https://github.com/huggingface/transformers/pull/23153\n",
    "            decoder_input_ids = labels\n",
    "\n",
    "        question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n",
    "        ### END COPY-PASTE ###\n",
    "        \n",
    "        # Add top down attention\n",
    "        image_attn = self.top_down_attn(image_embeds, question_embeds)\n",
    "        question_embeds = question_embeds * image_attn\n",
    "\n",
    "        ### START COPY-PASTE ###\n",
    "        answer_output = self.text_decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=question_embeds,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=return_dict,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        if labels is not None:\n",
    "            decoder_loss = answer_output.loss.mean() if return_dict else answer_output[0].mean()\n",
    "        else:\n",
    "            decoder_loss = None\n",
    "\n",
    "        if not return_dict:\n",
    "            outputs = (decoder_loss, image_embeds, vision_outputs[0]) + vision_outputs[2:]\n",
    "            return tuple(output for output in outputs if output is not None)\n",
    "\n",
    "        return BlipTextVisionModelOutput(\n",
    "            loss=decoder_loss,\n",
    "            image_embeds=image_embeds,\n",
    "            last_hidden_state=vision_outputs.last_hidden_state,\n",
    "            hidden_states=vision_outputs.hidden_states,\n",
    "            attentions=vision_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e001ab-a730-49b0-ab06-73af65d561ed",
   "metadata": {},
   "source": [
    "### ViT + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb06351c-2954-4587-9021-c66fc553e2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Developed based on https://github.com/huggingface/transformers/blob/main/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py\n",
    "class VisionTextEncoderDecoderConfig(PretrainedConfig):\n",
    "    model_type = \"vision-text-encoder-decoder\"\n",
    "    sub_configs = {\"vision_encoder\": AutoConfig, \"text_encoder\": AutoConfig, \"decoder\": AutoConfig}\n",
    "    has_no_defaults_at_init = True\n",
    "    \n",
    "    def __init__(self, use_top_down_attn=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # if \"vision_encoder\" not in kwargs or \"text_encoder\" not in kwargs or \"decoder\" not in kwargs:\n",
    "        #     raise ValueError(\n",
    "        #         f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n",
    "        #         f\"not all of `vision_encoder`, `text_encoder`, and `decoder` sub-configurations are passed, but only {kwargs}\"\n",
    "        #     )\n",
    "\n",
    "        vision_encoder_config = kwargs.pop(\"vision_encoder\", None)\n",
    "        if vision_encoder_config:\n",
    "            vision_encoder_model_type = vision_encoder_config.pop(\"model_type\", None)\n",
    "        text_encoder_config = kwargs.pop(\"text_encoder\", None)\n",
    "        if text_encoder_config:\n",
    "            text_encoder_model_type = text_encoder_config.pop(\"model_type\", None)\n",
    "        decoder_config = kwargs.pop(\"decoder\", None)\n",
    "        if decoder_config:\n",
    "            decoder_model_type = decoder_config.pop(\"model_type\", None)\n",
    "\n",
    "        if vision_encoder_config and vision_encoder_model_type:\n",
    "            self.vision_encoder = AutoConfig.for_model(vision_encoder_model_type, **vision_encoder_config)\n",
    "        if text_encoder_config and text_encoder_model_type:\n",
    "            self.text_encoder = AutoConfig.for_model(text_encoder_model_type, **text_encoder_config)\n",
    "        if decoder_config and decoder_model_type:\n",
    "            if decoder_model_type == 'blip_text_model':\n",
    "                # Load from finetuned checkpoints\n",
    "                self.decoder = BlipTextConfig(**decoder_config)\n",
    "            else:\n",
    "                self.decoder = AutoConfig.for_model(decoder_model_type, **decoder_config).text_config\n",
    "        self.is_encoder_decoder = True\n",
    "        \n",
    "        self.use_top_down_attn = use_top_down_attn\n",
    "\n",
    "    @classmethod\n",
    "    def from_encoder_decoder_configs(\n",
    "        cls, vision_encoder_config: PretrainedConfig, text_encoder_config: PretrainedConfig, \n",
    "        decoder_config: PretrainedConfig, **kwargs\n",
    "    ) -> PretrainedConfig:\n",
    "        r\"\"\"\n",
    "        Instantiate a [`VisionTextEncoderDecoderConfig`] (or a derived class) from two pre-trained encoder model\n",
    "        configurations and a decoder model configuration.\n",
    "\n",
    "        Returns:\n",
    "            [`VisionTextEncoderDecoderConfig`]: An instance of a configuration object\n",
    "        \"\"\"\n",
    "        logger.info(\"Setting `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\")\n",
    "        decoder_config.is_decoder = True\n",
    "        decoder_config.add_cross_attention = True\n",
    "\n",
    "        return cls(vision_encoder=vision_encoder_config.to_dict(), text_encoder=text_encoder_config.to_dict(), decoder=decoder_config.to_dict(), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1351765b-c131-49fe-9068-4fc865037826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionReducer(nn.Module):\n",
    "    def __init__(self, input_size=577, output_size=512):\n",
    "        super().__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(input_size, output_size))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_scores = self.softmax(self.attention_weights)  # Normalize scores\n",
    "        reduced_x = torch.matmul(x.permute(0, 2, 1), attn_scores,).permute(0, 2, 1)  \n",
    "        return reduced_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6d33e5e-810a-4a59-a0d4-bdaecb4d3091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Developed based on https://github.com/huggingface/transformers/blob/main/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py\n",
    "class VisionTextEncoderDecoderModelForQuestionAnswering(PreTrainedModel, GenerationMixin):\n",
    "    config_class = VisionTextEncoderDecoderConfig\n",
    "    base_model_prefix = \"vision_text_encoder_decoder\"\n",
    "    main_input_name = \"pixel_values\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _tied_weights_keys = [\"decoder.cls.predictions.decoder.bias\"]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Optional[PretrainedConfig] = None,\n",
    "        vision_encoder: Optional[PreTrainedModel] = None,\n",
    "        text_encoder: Optional[PreTrainedModel] = None,\n",
    "        decoder: Optional[PreTrainedModel] = None,\n",
    "    ):\n",
    "        if config is None and (vision_encoder is None or vision_encoder is None or decoder is None):\n",
    "            raise ValueError(\"Either a configuration or an encoder and a decoder has to be provided.\")\n",
    "        if config is None:\n",
    "            config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(vision_encoder.config, text_encoder.config, decoder.config)\n",
    "        else:\n",
    "            if not isinstance(config, self.config_class):\n",
    "                raise ValueError(f\"Config: {config} has to be of type {self.config_class}\")\n",
    "\n",
    "        if config.decoder.cross_attention_hidden_size is not None:\n",
    "            if config.decoder.cross_attention_hidden_size != config.encoder.hidden_size:\n",
    "                raise ValueError(\n",
    "                    \"If `cross_attention_hidden_size` is specified in the decoder's configuration, it has to be equal\"\n",
    "                    f\" to the encoder's `hidden_size`. Got {config.decoder.cross_attention_hidden_size} for\"\n",
    "                    f\" `config.decoder.cross_attention_hidden_size` and {config.encoder.hidden_size} for\"\n",
    "                    \" `config.encoder.hidden_size`.\"\n",
    "                )\n",
    "\n",
    "        # initialize with config\n",
    "        # make sure input & output embeddings is not tied\n",
    "        config.tie_word_embeddings = False\n",
    "        super().__init__(config)\n",
    "\n",
    "        if vision_encoder is None:\n",
    "            vision_encoder = AutoModel.from_config(config.vision_encoder)\n",
    "            \n",
    "        if text_encoder is None:\n",
    "            text_encoder = AutoModel.from_config(config.text_encoder)\n",
    "\n",
    "        if decoder is None:\n",
    "            if isinstance(config.decoder, BlipTextConfig):\n",
    "                decoder_pretrained_config = config.decoder\n",
    "            else:\n",
    "                decoder_pretrained_config = config.decoder.text_config\n",
    "            decoder = BlipTextLMHeadModel(decoder_pretrained_config)\n",
    "\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        if self.vision_encoder.config.to_dict() != self.config.vision_encoder.to_dict():\n",
    "            logger.warning(\n",
    "                f\"Config of the encoder: {self.vision_encoder.__class__} is overwritten by shared encoder config:\"\n",
    "                f\" {self.config.vision_encoder}\"\n",
    "            )\n",
    "        if self.text_encoder.config.to_dict() != self.config.text_encoder.to_dict():\n",
    "            logger.warning(\n",
    "                f\"Config of the encoder: {self.text_encoder.__class__} is overwritten by shared encoder config:\"\n",
    "                f\" {self.config.text_encoder}\"\n",
    "            )\n",
    "        if self.decoder.config.to_dict() != self.config.decoder.to_dict():\n",
    "            logger.warning(\n",
    "                f\"Config of the decoder: {self.decoder.__class__} is overwritten by shared decoder config:\"\n",
    "                f\" {self.config.decoder}\"\n",
    "            )\n",
    "\n",
    "        # make sure that the individual model's config refers to the shared config\n",
    "        # so that the updates to the config will be synced\n",
    "        self.config.vision_encoder._attn_implementation = self.vision_encoder.config._attn_implementation\n",
    "        self.config.text_encoder._attn_implementation = self.text_encoder.config._attn_implementation\n",
    "        self.config.decoder._attn_implementation = self.decoder.config._attn_implementation\n",
    "        self.vision_encoder.config = self.config.vision_encoder\n",
    "        self.text_encoder.config = self.config.text_encoder\n",
    "        self.decoder.config = self.config.decoder\n",
    "        \n",
    "        if self.config.use_top_down_attn:\n",
    "            self.top_down_attn = TopDownAttention(\n",
    "                image_feature_dim=self.vision_encoder.config.hidden_size, \n",
    "                question_dim=self.text_encoder.config.hidden_size,\n",
    "                hidden_dim=self.text_encoder.config.hidden_size,\n",
    "            )\n",
    "            enc_feat_dim = self.text_encoder.config.hidden_size\n",
    "        else:\n",
    "            self.vision_feat_reduction_attn = AttentionReducer()\n",
    "            enc_feat_dim = self.vision_encoder.config.hidden_size + self.text_encoder.config.hidden_size\n",
    "            \n",
    "        # encoder outputs need to be projected to different dimension for decoder\n",
    "        self.enc_to_dec_proj = nn.Linear(enc_feat_dim, self.decoder.config.hidden_size)\n",
    "        \n",
    "        self.decoder_pad_token_id = (\n",
    "            config.decoder.pad_token_id\n",
    "            if not hasattr(config, \"decoder_pad_token_id\")\n",
    "            else config.decoder_pad_token_id\n",
    "        )\n",
    "        self.decoder_start_token_id = (\n",
    "            config.decoder.bos_token_id\n",
    "            if not hasattr(config, \"decoder_start_token_id\")\n",
    "            else config.decoder_start_token_id\n",
    "        )\n",
    "        \n",
    "    @classmethod\n",
    "    def from_encoder_decoder_pretrained(\n",
    "        cls,\n",
    "        vision_encoder_pretrained_model_name_or_path: Optional[str] = None,\n",
    "        text_encoder_pretrained_model_name_or_path: Optional[str] = None,\n",
    "        decoder_pretrained_model_name_or_path: Optional[str] = None,\n",
    "        *model_args,\n",
    "        **kwargs,\n",
    "    ) -> PreTrainedModel:\n",
    "        \"\"\"\n",
    "        Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model\n",
    "        checkpoints.\n",
    "\n",
    "\n",
    "        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n",
    "        the model, you need to first set it back in training mode with `model.train()`.\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs_vision_encoder = {\n",
    "            argument[len(\"vision_encoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"vision_encoder_\")\n",
    "        }\n",
    "        \n",
    "        kwargs_text_encoder = {\n",
    "            argument[len(\"text_encoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"text_encoder_\")\n",
    "        }\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        # remove encoder, decoder kwargs from kwargs\n",
    "        for key in kwargs_vision_encoder.keys():\n",
    "            del kwargs[\"vision_encoder_\" + key]\n",
    "        for key in kwargs_text_encoder.keys():\n",
    "            del kwargs[\"text_encoder_\" + key]\n",
    "        for key in kwargs_decoder.keys():\n",
    "            del kwargs[\"decoder_\" + key]\n",
    "\n",
    "        # Load and initialize the encoder and decoder\n",
    "        # The distinction between encoder and decoder at the model level is made\n",
    "        # by the value of the flag `is_decoder` that we need to set correctly.\n",
    "        vision_encoder = kwargs_vision_encoder.pop(\"model\", None)\n",
    "        if vision_encoder is None:\n",
    "            if vision_encoder_pretrained_model_name_or_path is None:\n",
    "                raise ValueError(\n",
    "                    \"If `vision_encoder_model` is not defined as an argument, a `vision_encoder_pretrained_model_name_or_path` has \"\n",
    "                    \"to be defined.\"\n",
    "                )\n",
    "\n",
    "            if \"config\" not in kwargs_vision_encoder:\n",
    "                vision_encoder_config, kwargs_vision_encoder = AutoConfig.from_pretrained(\n",
    "                    vision_encoder_pretrained_model_name_or_path, **kwargs_vision_encoder, return_unused_kwargs=True\n",
    "                )\n",
    "\n",
    "                if vision_encoder_config.is_decoder is True or vision_encoder_config.add_cross_attention is True:\n",
    "                    logger.info(\n",
    "                        f\"Initializing {vision_encoder_pretrained_model_name_or_path} as a encoder model \"\n",
    "                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n",
    "                    )\n",
    "                    vision_encoder_config.is_decoder = False\n",
    "                    vision_encoder_config.add_cross_attention = False\n",
    "\n",
    "                kwargs_vision_encoder[\"config\"] = vision_encoder_config\n",
    "\n",
    "            vision_encoder = AutoModel.from_pretrained(vision_encoder_pretrained_model_name_or_path, *model_args, **kwargs_vision_encoder)\n",
    "        \n",
    "        text_encoder = kwargs_text_encoder.pop(\"model\", None)\n",
    "        if text_encoder is None:\n",
    "            if text_encoder_pretrained_model_name_or_path is None:\n",
    "                raise ValueError(\n",
    "                    \"If `text_encoder_model` is not defined as an argument, a `text_encoder_pretrained_model_name_or_path` has \"\n",
    "                    \"to be defined.\"\n",
    "                )\n",
    "\n",
    "            if \"config\" not in kwargs_text_encoder:\n",
    "                text_encoder_config, kwargs_text_encoder = AutoConfig.from_pretrained(\n",
    "                    text_encoder_pretrained_model_name_or_path, **kwargs_text_encoder, return_unused_kwargs=True\n",
    "                )\n",
    "\n",
    "                if text_encoder_config.is_decoder is True or text_encoder_config.add_cross_attention is True:\n",
    "                    logger.info(\n",
    "                        f\"Initializing {text_encoder_pretrained_model_name_or_path} as a encoder model \"\n",
    "                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n",
    "                    )\n",
    "                    text_encoder_config.is_decoder = False\n",
    "                    text_encoder_config.add_cross_attention = False\n",
    "\n",
    "                kwargs_text_encoder[\"config\"] = text_encoder_config\n",
    "\n",
    "            text_encoder = AutoModel.from_pretrained(text_encoder_pretrained_model_name_or_path, *model_args, **kwargs_text_encoder)\n",
    "\n",
    "        decoder = kwargs_decoder.pop(\"model\", None)\n",
    "        if decoder is None:\n",
    "            if decoder_pretrained_model_name_or_path is None:\n",
    "                raise ValueError(\n",
    "                    \"If `decoder_model` is not defined as an argument, a `decoder_pretrained_model_name_or_path` has \"\n",
    "                    \"to be defined.\"\n",
    "                )\n",
    "\n",
    "            if \"config\" not in kwargs_decoder:\n",
    "                decoder_config, kwargs_decoder = AutoConfig.from_pretrained(\n",
    "                    decoder_pretrained_model_name_or_path, **kwargs_decoder, return_unused_kwargs=True\n",
    "                )\n",
    "\n",
    "                if decoder_config.is_decoder is False or decoder_config.add_cross_attention is False:\n",
    "                    logger.info(\n",
    "                        f\"Initializing {decoder_pretrained_model_name_or_path} as a decoder model. Cross attention\"\n",
    "                        f\" layers are added to {decoder_pretrained_model_name_or_path} and randomly initialized if\"\n",
    "                        f\" {decoder_pretrained_model_name_or_path}'s architecture allows for cross attention layers.\"\n",
    "                    )\n",
    "                    decoder_config.is_decoder = True\n",
    "                    decoder_config.add_cross_attention = True\n",
    "\n",
    "                kwargs_decoder[\"config\"] = decoder_config\n",
    "\n",
    "            if kwargs_decoder[\"config\"].is_decoder is False or kwargs_decoder[\"config\"].add_cross_attention is False:\n",
    "                logger.warning(\n",
    "                    f\"Decoder model {decoder_pretrained_model_name_or_path} is not initialized as a decoder. \"\n",
    "                    f\"In order to initialize {decoder_pretrained_model_name_or_path} as a decoder, \"\n",
    "                    \"make sure that the attributes `is_decoder` and `add_cross_attention` of `decoder_config` \"\n",
    "                    \"passed to `.from_encoder_decoder_pretrained(...)` are set to `True` or do not pass a \"\n",
    "                    \"`decoder_config` to `.from_encoder_decoder_pretrained(...)`\"\n",
    "                )\n",
    "\n",
    "            decoder = BlipForQuestionAnswering.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)\n",
    "\n",
    "        # instantiate config with corresponding kwargs\n",
    "        config = VisionTextEncoderDecoderConfig.from_encoder_decoder_configs(vision_encoder.config, text_encoder.config, decoder.config, **kwargs)\n",
    "        decoder = decoder.text_decoder\n",
    "\n",
    "        # make sure input & output embeddings is not tied\n",
    "        config.tie_word_embeddings = False\n",
    "        return cls(vision_encoder=vision_encoder, text_encoder=text_encoder, decoder=decoder, config=config)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple[torch.FloatTensor], BlipTextVisionModelOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # num_items_in_batch is only needed for loss computation\n",
    "        num_items_in_batch = kwargs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        kwargs_vision_encoder = {argument: value for argument, value in kwargs.items() if argument.startswith(\"vision_encoder_\")}\n",
    "        kwargs_text_encoder = {argument: value for argument, value in kwargs.items() if argument.startswith(\"text_encoder_\")}\n",
    "\n",
    "        kwargs_decoder = {\n",
    "            argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n",
    "        }\n",
    "\n",
    "        vision_encoder_outputs = self.vision_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs_vision_encoder,\n",
    "        )\n",
    "        text_encoder_outputs = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs_text_encoder,\n",
    "        )\n",
    "            \n",
    "        vision_encoder_hidden_states = vision_encoder_outputs[0]\n",
    "        text_encoder_hidden_states = text_encoder_outputs[0]\n",
    "        if self.config.use_top_down_attn:\n",
    "            image_attn = self.top_down_attn(vision_encoder_hidden_states, text_encoder_hidden_states)\n",
    "            encoder_hidden_states = text_encoder_hidden_states * image_attn\n",
    "        else:\n",
    "            vision_encoder_hidden_states_proj = self.vision_feat_reduction_attn(vision_encoder_hidden_states)\n",
    "            encoder_hidden_states = torch.concat([vision_encoder_hidden_states_proj, text_encoder_hidden_states], dim=-1)\n",
    "\n",
    "        encoder_hidden_states = self.enc_to_dec_proj(encoder_hidden_states)\n",
    "\n",
    "        # else:\n",
    "        encoder_attention_mask = None\n",
    "\n",
    "        if labels is not None and decoder_input_ids is None:\n",
    "            # labels are already shifted right, see: https://github.com/huggingface/transformers/pull/23153\n",
    "            decoder_input_ids = labels\n",
    "\n",
    "        # Decode\n",
    "        answer_output = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            return_dict=return_dict,\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = answer_output.loss.mean() if return_dict else answer_output[0].mean()\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        if not return_dict:\n",
    "            if loss is not None:\n",
    "                return (loss,) + answer_output + vision_encoder_outputs + text_encoder_outputs\n",
    "            else:\n",
    "                return answer_output + vision_encoder_outputs + text_encoder_outputs\n",
    "\n",
    "        return BlipTextVisionModelOutput(\n",
    "            loss=loss,\n",
    "            image_embeds=vision_encoder_hidden_states,\n",
    "            last_hidden_state=vision_encoder_hidden_states,\n",
    "            hidden_states=vision_encoder_outputs.hidden_states,\n",
    "            attentions=vision_encoder_outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        interpolate_pos_encoding: bool = False,\n",
    "        **generate_kwargs,\n",
    "    ) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Overrides *generate* function to be able to use the model as a conditional generator\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (*torch.LongTensor* of shape *(batch_size, sequence_length)*):\n",
    "                The sequence used as a prompt for the generation.\n",
    "            pixel_values (*torch.FloatTensor* of shape *(batch_size, num_channels, image_height, image_width)*:\n",
    "                Input image to be processed\n",
    "            attention_mask (*torch.LongTensor* of shape *(batch_size, sequence_length)*, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`. `1` for\n",
    "                tokens that are NOT MASKED, `0` for MASKED tokens.\n",
    "            **generate_kwargs:\n",
    "                Additional arguments passed to the *generate* function of the decoder\n",
    "        \"\"\"\n",
    "        vision_outputs = self.vision_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
    "        )\n",
    "\n",
    "        image_embeds = vision_outputs[0]\n",
    "\n",
    "        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "        question_outputs = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "        )\n",
    "\n",
    "        question_embeds = question_outputs[0]\n",
    "        \n",
    "        if self.config.use_top_down_attn:\n",
    "            image_attn = self.top_down_attn(image_embeds, question_embeds)\n",
    "            question_embeds = question_embeds * image_attn\n",
    "        else:\n",
    "            image_embeds_proj = self.vision_feat_reduction_attn(image_embeds)\n",
    "            question_embeds = torch.concat([image_embeds_proj, question_embeds], dim=-1)\n",
    "\n",
    "        question_embeds = self.enc_to_dec_proj(question_embeds)\n",
    "\n",
    "        question_attention_mask = torch.ones(\n",
    "            question_embeds.size()[:-1], dtype=torch.long, device=question_embeds.device\n",
    "        )\n",
    "\n",
    "        bos_ids = torch.full(\n",
    "            (question_embeds.size(0), 1), fill_value=self.decoder_start_token_id, device=question_embeds.device\n",
    "        )\n",
    "\n",
    "        outputs = self.decoder.generate(\n",
    "            input_ids=bos_ids,\n",
    "            eos_token_id=self.config.decoder.sep_token_id,\n",
    "            pad_token_id=self.config.decoder.pad_token_id,\n",
    "            encoder_hidden_states=question_embeds,\n",
    "            encoder_attention_mask=question_attention_mask,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0c14c-df63-4dc3-a91a-24f8a82b064c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd3ce4-4a93-41e7-a1f1-b98eb5c0ac19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment 1. Finetune pretrained BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef55ba07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BlipForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a3efa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./ckpt-exp1',\n",
    "    per_device_train_batch_size=12,\n",
    "    dataloader_num_workers=12,\n",
    "    # num_train_epochs=2,\n",
    "    max_steps=7500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8238025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 20:09:52,483] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_wrapped,\n",
    "    eval_dataset=val_wrapped,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7863d69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 4:15:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.190690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.179700</td>\n",
       "      <td>0.188632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.181122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.174907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.172125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.171344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.167277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.174912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.168483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.174329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.170734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.162527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.164866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>0.158236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.157603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['text_decoder.cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.26640469783147175, metrics={'train_runtime': 15355.6321, 'train_samples_per_second': 5.861, 'train_steps_per_second': 0.488, 'total_flos': 9.317411278848e+16, 'train_loss': 0.26640469783147175, 'epoch': 2.0281233098972415})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba54ca6-31b4-4a10-807d-7f74410b31a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f6593-e858-4b40-84cb-090e446a4b9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment 2. Finetune BLIP + top-down attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b45a69c-5aab-438b-8ba5-df3ee05ce634",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = BlipForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ca6541-254f-473d-ade5-9b93849ec6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blip_top_down_attn = CustomBlipForQuestionAnswering(pretrained_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1c9241-b557-4754-9041-07ff2cac3aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['top_down_attn.attention_layer.0.weight', 'top_down_attn.attention_layer.0.bias', 'top_down_attn.attention_layer.1.weight', 'top_down_attn.attention_layer.1.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip_top_down_attn.load_state_dict(pretrained_model.state_dict(), strict=False)  # Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b7dc99-8a18-448f-a362-888331744af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pretrained_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64088f5a-14f0-4510-b870-e0837fb86ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./ckpt-exp2',\n",
    "    per_device_train_batch_size=12,\n",
    "    dataloader_num_workers=12,\n",
    "    # num_train_epochs=2,\n",
    "    max_steps=7500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c756dc23-a3b9-47a9-bf77-aacee88e8ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-23 08:02:05,700] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=blip_top_down_attn,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_wrapped,\n",
    "    eval_dataset=val_wrapped,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c14474-b84c-48f4-b851-308ba5085f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 4:25:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.250734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.199658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.200400</td>\n",
       "      <td>0.200328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.207600</td>\n",
       "      <td>0.184504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.186701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.179315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.186822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.174211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.178650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.182654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.172053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.171292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.165728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.165230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['text_decoder.cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.31387046438852945, metrics={'train_runtime': 15956.1163, 'train_samples_per_second': 5.64, 'train_steps_per_second': 0.47, 'total_flos': 9.366376108032e+16, 'train_loss': 0.31387046438852945, 'epoch': 2.0281233098972415})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e24005b-863b-4205-a377-a701de6d9485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del blip_top_down_attn\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af230a-b8bf-41b9-a41b-fe030cc7671b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment 3. Finetune ViT + BERT + BLIP decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7913c8-2ff1-4737-a65b-aeb0de03dab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionTextEncoderDecoderModelForQuestionAnswering.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-384\", \"google-bert/bert-base-uncased\", model_checkpoint\n",
    ")\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d19be67c-2a51-4ac4-a5b4-becd919139ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./ckpt-exp3',\n",
    "    per_device_train_batch_size=12,\n",
    "    dataloader_num_workers=12,\n",
    "    # num_train_epochs=2,\n",
    "    max_steps=7500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7e272a3-b1db-4766-88d3-d49f41179675",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-23 20:31:56,468] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_wrapped,\n",
    "    eval_dataset=val_wrapped,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68f19537-a87c-4da9-8b28-0a6886cea67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 2:52:30, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.509662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.450919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.424438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.394656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.383725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.390100</td>\n",
       "      <td>0.372361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.364684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.363574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.359079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.297700</td>\n",
       "      <td>0.353870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.354201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.350843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.346748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.343687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.342804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.47820610466003416, metrics={'train_runtime': 10354.3241, 'train_samples_per_second': 8.692, 'train_steps_per_second': 0.724, 'total_flos': 7.436090360070144e+19, 'train_loss': 0.47820610466003416, 'epoch': 2.0281233098972415})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e82b612-bd8c-4fce-8461-8b50570f129f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa59c3-1106-4123-98be-7ec4edc13b8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Experiment 4. Finetune ViT + BERT + top-down attention + BLIP decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78b52188-8104-45cc-837f-35c375a02d02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionTextEncoderDecoderModelForQuestionAnswering.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-384\", \"google-bert/bert-base-uncased\", model_checkpoint, use_top_down_attn=True,\n",
    ")\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c6ff182-820f-4974-9963-4a95686370d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./ckpt-exp4',\n",
    "    per_device_train_batch_size=12,\n",
    "    dataloader_num_workers=12,\n",
    "    # num_train_epochs=2,\n",
    "    max_steps=7500,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ffc788f-bdd0-4618-b4a6-87f9a04d0481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 09:25:14,874] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libstdc++.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::runtime_error::~runtime_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__gxx_personality_v0@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::tellp()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::steady_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for bool@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_logic_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::logic_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::~locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_end_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::logic_error::~logic_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__si_class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new[](unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak_hard()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::basic_streambuf(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned short@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::resize(unsigned long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ctype<char>::_M_widen_init() const@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_invalid_argument(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::operator=(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::_M_cache_locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_free_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::notify_one()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::~Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_pure_virtual@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::flush()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for __cxxabiv1::__class_type_info@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_rethrow@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_fstream<char, std::char_traits<char> >::~basic_fstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::compare(char const*) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::chrono::_V2::system_clock::now()@GLIBCXX_3.4.19'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_ifstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Hash_bytes(void const*, unsigned long, unsigned long)@CXXABI_1.3.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long long>(long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for char*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_Prime_rehash_policy::_M_need_rehash(unsigned long, unsigned long, unsigned long) const@GLIBCXX_3.4.18'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long>(unsigned long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::~ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::range_error::~range_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::~__basic_file()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_acquire@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<bool>(bool)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::overflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::range_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_filebuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete[](void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(unsigned long, char, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_transfer(std::__detail::_List_node_base*, std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::exception@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_M_destroy(std::allocator<wchar_t> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream& std::istream::_M_extract<double>(double&)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_fstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::basic_ifstream(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator new(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::append(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::domain_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::put(char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_alloc()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_thread_atexit@CXXABI_1.3.7'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_increment(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::~basic_ifstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::Init::Init()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::basic_filebuf()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::domain_error::~domain_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cerr@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::invalid_argument@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(std::string const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_rebalance_for_erase(std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_hook(std::__detail::_List_node_base*)@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__detail::_List_node_base::_M_unhook()@GLIBCXX_3.4.15'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<char, std::char_traits<char> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::locale::locale(std::locale const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_istringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::exception::~exception()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_create(unsigned long, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__basic_file<char>::is_open() const@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::swap(std::string&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::basic_streambuf(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::init(std::basic_streambuf<char, std::char_traits<char> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_bad_cast()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<char, std::char_traits<char> >::clear(std::_Ios_Iostate)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >::operator=(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `operator delete(void*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream::operator<<(int)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_Rep::_M_destroy(std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_iostream<wchar_t, std::char_traits<wchar_t> >::~basic_iostream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::runtime_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ofstream<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `VTT for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<long>(long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::get()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::operator<< <std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::out_of_range::~out_of_range()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::length_error::~length_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ostream<char, std::char_traits<char> >& std::__ostream_insert<char, std::char_traits<char> >(std::basic_ostream<char, std::char_traits<char> >&, char const*, long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::invalid_argument::~invalid_argument()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::swap(std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::cout@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<unsigned long long>(unsigned long long)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for int*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<void const*>(void const*)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::underflow_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_streambuf<char, std::char_traits<char> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for std::out_of_range@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_allocate_exception@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_ios<wchar_t, std::char_traits<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for void const*@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ios<wchar_t, std::char_traits<wchar_t> >::init(std::basic_streambuf<wchar_t, std::char_traits<wchar_t> >*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::reserve(unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_begin_catch@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_Rep::_S_empty_rep_storage@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::_M_leak()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::open(char const*, std::_Ios_Openmode)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >::_M_sync(wchar_t*, unsigned long, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::istream::getline(char*, long, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringstream<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::condition_variable::~condition_variable()@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::basic_stringbuf<wchar_t, std::char_traits<wchar_t>, std::allocator<wchar_t> >@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::insert(unsigned long, char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::string::assign(char const*, unsigned long)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for unsigned char@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ios_base::ios_base()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_out_of_range(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::overflow_error::~overflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_length_error(char const*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::__throw_system_error(int)@GLIBCXX_3.4.11'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ofstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::ostream& std::ostream::_M_insert<double>(double)@GLIBCXX_3.4.9'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_streambuf<char, std::char_traits<char> >::operator=(std::basic_streambuf<char, std::char_traits<char> > const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `typeinfo for long long@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_string<char, std::char_traits<char>, std::allocator<char> >::basic_string(char const*, unsigned long, std::allocator<char> const&)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_ifstream<char, std::char_traits<char> >::close()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_guard_release@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `__cxa_throw@CXXABI_1.3'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::underflow_error::~underflow_error()@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::_Rb_tree_decrement(std::_Rb_tree_node_base*)@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `vtable for std::length_error@GLIBCXX_3.4'\n",
      "/home/ray/anaconda3/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `std::basic_filebuf<char, std::char_traits<char> >::~basic_filebuf()@GLIBCXX_3.4'\n",
      "collect2: error: ld returned 1 exit status\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_wrapped,\n",
    "    eval_dataset=val_wrapped,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ed85e6-4843-45f8-96d2-cece2015cd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 2:57:24, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.570200</td>\n",
       "      <td>0.500403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.446343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.420005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.388400</td>\n",
       "      <td>0.400496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.389973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.378734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.370872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.368733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.364590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.363969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>0.363603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.359058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.356666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.353539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.353495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['decoder.cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.48927564315795896, metrics={'train_runtime': 10648.1994, 'train_samples_per_second': 8.452, 'train_steps_per_second': 0.704, 'total_flos': 7.457249281572864e+19, 'train_loss': 0.48927564315795896, 'epoch': 2.0281233098972415})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ee28-3f65-4e43-83db-ab89deaca626",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b4274-f025-4442-a28a-0dd09c84b66c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75ad3a-99da-4a2d-98c7-cb2ceae2bee5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b61ee2-9735-432f-9504-e77e271d53b0",
   "metadata": {},
   "source": [
    "All 4 models are evaluated on the same validation dataset split during training, so we can use validation loss to evaluate their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ecefe1a-78e2-4193-bfb4-3331fe240f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps = np.arange(500, 8000, 500)\n",
    "blip = [0.190690, 0.188632, 0.181122, 0.174907, 0.172125, 0.171344, 0.167277, 0.174912, 0.168483, 0.174329, 0.170734, 0.162527, 0.164866, 0.158236, 0.157603]\n",
    "blip_tda = [0.250734, 0.218200, 0.199658, 0.200328, 0.184504, 0.186701, 0.179315, 0.186822, 0.174211, 0.178650, 0.182654, 0.172053, 0.171292, 0.165728, 0.165230]\n",
    "vit_bert = [0.509662, 0.450919, 0.424438, 0.394656, 0.383725, 0.372361, 0.364684, 0.363574, 0.359079, 0.353870, 0.354201, 0.350843, 0.346748, 0.343687, 0.342804]\n",
    "vit_bert_tda = [0.500403, 0.446343, 0.420005, 0.400496, 0.389973, 0.378734, 0.370872, 0.368733, 0.364590, 0.363969, 0.363603, 0.359058, 0.356666, 0.353539, 0.353495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbeac47e-44cc-4f65-94f5-d02d3eb2ba15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, blip, label=\"BLIP\", marker=\"o\", color=\"blue\")\n",
    "plt.plot(steps, blip_tda, label=\"BLIP + top-down attention\", marker=\"s\", color=\"red\")\n",
    "plt.plot(steps, vit_bert, label=\"ViT + BERT\", marker=\"^\", color=\"green\")\n",
    "plt.plot(steps, vit_bert_tda, label=\"ViT + BERT + top-down attention\", marker=\"d\", color=\"purple\")\n",
    "\n",
    "plt.title(\"Validation Loss during Training\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c582e3e-f367-416e-a09b-e10989e1ae4a",
   "metadata": {},
   "source": [
    "It's clear that vanilla BLIP has the best performance because it's the first to converge, and it converges to a lower validation loss. Among the rest, BLIP with top down attention converges at a pace only a little slower than vanilla BLIP, and reaches a slightly higher loss in the end. The ViT + BERT + BLIP decoder model has worse performance, where it converges the most slowly, and reaches higher loss in the end. The ViT + BERT + top-down attention + BLIP decoder model converges faster than its counterpart without top-down attention, but arrives at the highest loss in the end.\n",
    "\n",
    "By looking at the plots, I have the following findings:\n",
    "- Adding top-down attention will help vision and text encoders initially, when they are not pretrained by VLP, but may lead to slower convergence\n",
    "- Adding top-down attention will not help vision and text encoders pretrained by VLP, and at the same time it will slow down convergence\n",
    "- VLP pretrained vision and text encoders perform better than separately pretrained ones plus top-down attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25e6c7-97c2-4fcd-80de-a573aee84adc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VQA accuracy metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073edb9a-24c0-4069-86e5-bdc0530fd141",
   "metadata": {},
   "source": [
    "Although validation losses tell a good story, it's better to compute some metrics on a completely unseen dataset to gather more information.\n",
    "\n",
    "To do that, I choose to run [VQA accuracy](https://arxiv.org/pdf/1505.00468) on 1000 examples of the val split of the VQAv2 dataset. This split doesn't overlap with the one used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4821ebe8-9952-444b-bf57-4cb940607669",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd509397-3dd9-4c86-9162-f404ff9bacda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "test_dset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\", \n",
    "    split=\"validation[-1000:]\",\n",
    "    storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30cd5b7c-2c14-44bf-8b53-32a5e710350f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08d8cc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vqa_accuracy = load(\"Kamichanw/vqa_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c65d2568-6bc9-4d7b-b577-c58b14d2a7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(data, device):\n",
    "    if isinstance(data, Mapping):\n",
    "        return type(data)({k: prepare_input(v, device) for k, v in data.items()})\n",
    "    elif isinstance(data, (tuple, list)):\n",
    "        return type(data)(prepare_input(v, device) for v in data)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.to(device)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6039a3bb-669c-4bec-bfac-4b56a1a8d5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    refs = []\n",
    "    label_ids = []\n",
    "    preds = []\n",
    "    question_types = []\n",
    "    answer_types = []\n",
    "    for example in tqdm(test_dset):\n",
    "        inputs = processor(images=example['image'], text=example['question'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        label = processor(text=example['multiple_choice_answer'], return_tensors=\"pt\").input_ids\n",
    "        output = model.generate(**prepare_input(inputs, 'cuda:0'))\n",
    "        pred = processor.decode(output[0], skip_special_tokens=True)\n",
    "        refs.append([ans['answer'] for ans in example['answers']])\n",
    "        label_ids.append(label)\n",
    "        preds.append(pred)\n",
    "        question_types.append(example['question_type'])\n",
    "        answer_types.append(example['answer_type'])\n",
    "    return refs, label_ids, preds, question_types, answer_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f2f504-108e-47fb-8065-fa44815eea3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_vqa_accuracy(model):\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    with torch.no_grad():\n",
    "        refs, label_ids, preds, question_types, answer_types = test_model(model)\n",
    "    results = vqa_accuracy.compute(predictions=preds, references=refs, question_types=question_types, answer_types=answer_types)\n",
    "    return results, refs, label_ids, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb70254-e936-4472-96bf-67ec8bafbe30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f57699-3970-4f54-b6b9-954528e235a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blip = BlipForQuestionAnswering.from_pretrained('./ckpt-exp1/checkpoint-7500')\n",
    "blip_tda = CustomBlipForQuestionAnswering.from_pretrained('./ckpt-exp2/checkpoint-7500')\n",
    "vit_bert = VisionTextEncoderDecoderModelForQuestionAnswering.from_pretrained('./ckpt-exp3/checkpoint-7500')\n",
    "vit_bert_tda = VisionTextEncoderDecoderModelForQuestionAnswering.from_pretrained('./ckpt-exp4/checkpoint-7500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee80fd-f39a-46d9-ba97-dee3cb8c47bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddc79335-b929-40b8-a6f1-58f497b4c975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:40<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "blip_results, blip_refs, blip_label_ids, blip_preds = compute_vqa_accuracy(blip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6faedf6a-e1b2-435b-93ff-13629e8b6a14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 15.04it/s]\n"
     ]
    }
   ],
   "source": [
    "blip_tda_results, blip_tda_refs, blip_tda_label_ids, blip_tda_preds = compute_vqa_accuracy(blip_tda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a2cbe133-131d-4107-9bf1-6140b258d14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:03<00:00, 15.70it/s]\n"
     ]
    }
   ],
   "source": [
    "vit_bert_results, vit_bert_refs, vit_bert_label_ids, vit_bert_preds = compute_vqa_accuracy(vit_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3d89cfe-4e36-4927-9208-2f455b37cedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:06<00:00, 14.94it/s]\n"
     ]
    }
   ],
   "source": [
    "vit_bert_tda_results, vit_bert_tda_refs, vit_bert_tda_label_ids, vit_bert_tda_preds = compute_vqa_accuracy(vit_bert_tda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afd06592-c454-4e14-b5f6-b2d9343a9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    flat_dict = d.copy()\n",
    "    for key, sub_dict in d.items():\n",
    "        if isinstance(sub_dict, dict):\n",
    "            for sub_key, value in sub_dict.items():\n",
    "                flat_dict[f\"{key}_{sub_key}\"] = value  # Create new key format: \"ParentKey_SubKey\"\n",
    "            del flat_dict[key]  # Remove original nested dictionary\n",
    "    return flat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3d2399b-256a-47fb-b381-b326f977fc73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>perAnswerType_yes/no</th>\n",
       "      <th>perAnswerType_number</th>\n",
       "      <th>perAnswerType_other</th>\n",
       "      <th>perQuestionType_is the</th>\n",
       "      <th>perQuestionType_how many</th>\n",
       "      <th>perQuestionType_what is the person</th>\n",
       "      <th>perQuestionType_what</th>\n",
       "      <th>perQuestionType_what color are the</th>\n",
       "      <th>perQuestionType_is there</th>\n",
       "      <th>...</th>\n",
       "      <th>perQuestionType_are these</th>\n",
       "      <th>perQuestionType_who is</th>\n",
       "      <th>perQuestionType_could</th>\n",
       "      <th>perQuestionType_what is the color of the</th>\n",
       "      <th>perQuestionType_has</th>\n",
       "      <th>perQuestionType_is this an</th>\n",
       "      <th>perQuestionType_is that a</th>\n",
       "      <th>perQuestionType_what brand</th>\n",
       "      <th>perQuestionType_is this person</th>\n",
       "      <th>perQuestionType_do you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BLIP</th>\n",
       "      <td>78.68</td>\n",
       "      <td>95.994318</td>\n",
       "      <td>67.317073</td>\n",
       "      <td>69.733333</td>\n",
       "      <td>93.086420</td>\n",
       "      <td>75.274725</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>65.945946</td>\n",
       "      <td>85.789474</td>\n",
       "      <td>92.380952</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLIP + top-down attention</th>\n",
       "      <td>77.29</td>\n",
       "      <td>95.085227</td>\n",
       "      <td>64.796748</td>\n",
       "      <td>68.285714</td>\n",
       "      <td>91.481481</td>\n",
       "      <td>69.560440</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>66.081081</td>\n",
       "      <td>80.526316</td>\n",
       "      <td>96.190476</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViT + BERT</th>\n",
       "      <td>44.45</td>\n",
       "      <td>69.687500</td>\n",
       "      <td>30.894309</td>\n",
       "      <td>30.704762</td>\n",
       "      <td>72.716049</td>\n",
       "      <td>34.285714</td>\n",
       "      <td>18.571429</td>\n",
       "      <td>30.810811</td>\n",
       "      <td>55.789474</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ViT + BERT + top-down attention</th>\n",
       "      <td>40.29</td>\n",
       "      <td>66.107955</td>\n",
       "      <td>31.382114</td>\n",
       "      <td>25.066667</td>\n",
       "      <td>60.864198</td>\n",
       "      <td>35.604396</td>\n",
       "      <td>32.857143</td>\n",
       "      <td>22.702703</td>\n",
       "      <td>18.947368</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>78.75</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 overall  perAnswerType_yes/no  \\\n",
       "BLIP                               78.68             95.994318   \n",
       "BLIP + top-down attention          77.29             95.085227   \n",
       "ViT + BERT                         44.45             69.687500   \n",
       "ViT + BERT + top-down attention    40.29             66.107955   \n",
       "\n",
       "                                 perAnswerType_number  perAnswerType_other  \\\n",
       "BLIP                                        67.317073            69.733333   \n",
       "BLIP + top-down attention                   64.796748            68.285714   \n",
       "ViT + BERT                                  30.894309            30.704762   \n",
       "ViT + BERT + top-down attention             31.382114            25.066667   \n",
       "\n",
       "                                 perQuestionType_is the  \\\n",
       "BLIP                                          93.086420   \n",
       "BLIP + top-down attention                     91.481481   \n",
       "ViT + BERT                                    72.716049   \n",
       "ViT + BERT + top-down attention               60.864198   \n",
       "\n",
       "                                 perQuestionType_how many  \\\n",
       "BLIP                                            75.274725   \n",
       "BLIP + top-down attention                       69.560440   \n",
       "ViT + BERT                                      34.285714   \n",
       "ViT + BERT + top-down attention                 35.604396   \n",
       "\n",
       "                                 perQuestionType_what is the person  \\\n",
       "BLIP                                                      60.000000   \n",
       "BLIP + top-down attention                                 64.285714   \n",
       "ViT + BERT                                                18.571429   \n",
       "ViT + BERT + top-down attention                           32.857143   \n",
       "\n",
       "                                 perQuestionType_what  \\\n",
       "BLIP                                        65.945946   \n",
       "BLIP + top-down attention                   66.081081   \n",
       "ViT + BERT                                  30.810811   \n",
       "ViT + BERT + top-down attention             22.702703   \n",
       "\n",
       "                                 perQuestionType_what color are the  \\\n",
       "BLIP                                                      85.789474   \n",
       "BLIP + top-down attention                                 80.526316   \n",
       "ViT + BERT                                                55.789474   \n",
       "ViT + BERT + top-down attention                           18.947368   \n",
       "\n",
       "                                 perQuestionType_is there  ...  \\\n",
       "BLIP                                            92.380952  ...   \n",
       "BLIP + top-down attention                       96.190476  ...   \n",
       "ViT + BERT                                      70.000000  ...   \n",
       "ViT + BERT + top-down attention                 73.333333  ...   \n",
       "\n",
       "                                 perQuestionType_are these  \\\n",
       "BLIP                                                100.00   \n",
       "BLIP + top-down attention                           100.00   \n",
       "ViT + BERT                                          100.00   \n",
       "ViT + BERT + top-down attention                      78.75   \n",
       "\n",
       "                                 perQuestionType_who is  \\\n",
       "BLIP                                         100.000000   \n",
       "BLIP + top-down attention                    100.000000   \n",
       "ViT + BERT                                    10.000000   \n",
       "ViT + BERT + top-down attention               33.333333   \n",
       "\n",
       "                                 perQuestionType_could  \\\n",
       "BLIP                                        100.000000   \n",
       "BLIP + top-down attention                    96.666667   \n",
       "ViT + BERT                                   63.333333   \n",
       "ViT + BERT + top-down attention             100.000000   \n",
       "\n",
       "                                 perQuestionType_what is the color of the  \\\n",
       "BLIP                                                                100.0   \n",
       "BLIP + top-down attention                                           100.0   \n",
       "ViT + BERT                                                           32.5   \n",
       "ViT + BERT + top-down attention                                      47.5   \n",
       "\n",
       "                                 perQuestionType_has  \\\n",
       "BLIP                                           100.0   \n",
       "BLIP + top-down attention                      100.0   \n",
       "ViT + BERT                                       0.0   \n",
       "ViT + BERT + top-down attention                 50.0   \n",
       "\n",
       "                                 perQuestionType_is this an  \\\n",
       "BLIP                                                  100.0   \n",
       "BLIP + top-down attention                             100.0   \n",
       "ViT + BERT                                             82.5   \n",
       "ViT + BERT + top-down attention                        75.0   \n",
       "\n",
       "                                 perQuestionType_is that a  \\\n",
       "BLIP                                                 100.0   \n",
       "BLIP + top-down attention                            100.0   \n",
       "ViT + BERT                                           100.0   \n",
       "ViT + BERT + top-down attention                        0.0   \n",
       "\n",
       "                                 perQuestionType_what brand  \\\n",
       "BLIP                                                   50.0   \n",
       "BLIP + top-down attention                              50.0   \n",
       "ViT + BERT                                              0.0   \n",
       "ViT + BERT + top-down attention                         0.0   \n",
       "\n",
       "                                 perQuestionType_is this person  \\\n",
       "BLIP                                                      100.0   \n",
       "BLIP + top-down attention                                 100.0   \n",
       "ViT + BERT                                                100.0   \n",
       "ViT + BERT + top-down attention                           100.0   \n",
       "\n",
       "                                 perQuestionType_do you  \n",
       "BLIP                                              100.0  \n",
       "BLIP + top-down attention                         100.0  \n",
       "ViT + BERT                                          0.0  \n",
       "ViT + BERT + top-down attention                   100.0  \n",
       "\n",
       "[4 rows x 69 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [flatten_dict(d) for d in [blip_results, blip_tda_results, vit_bert_results, vit_bert_tda_results]]\n",
    "df = pd.DataFrame(data)\n",
    "df.index = ['BLIP', 'BLIP + top-down attention', 'ViT + BERT', 'ViT + BERT + top-down attention']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80a7e8-2b38-4f2f-80b5-5f2f4eb688ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The metrics mostly align with the findings from the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21fc0e-0f4f-4ea4-bd31-dccc2dbfc5c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01695561-db8d-4e07-b858-4b61e46491d1",
   "metadata": {},
   "source": [
    "Based on the findings in evaluation, I'm able to draw the following preliminary conclusions:\n",
    "- VLP is more effective in terms of aggregating vision and text features.\n",
    "  - The best performing model BLIP has two ways of VLP, **Multimodal Mixture of Encoder-Decoder (MED)** and **Captioning and Filtering (CapFilt)**\n",
    "  - CapFilt is designed to effectively utilize noisy web data, which can train the vision and text encoder, as well as the decoder, to encode and decode paired vision and text features, and the authors conduct ablation studies to demonstrate its effectiveness. \n",
    "  - MED directly couples vision features into text encoder, so that the text encoder will extract text features under the guidance of the vision features. The authors don't conduct ablation studies to explicitly demonstrate its effectiveness, but by comparing the vanilla BLIP and ViT + BERT + BLIP decoder, we can see how MED helps\n",
    "- Top-down attention can help when vision and text encoders are not trained with VLP, but it's only helpful in early training stages, and may result in slower convergence\n",
    "  - The top-down attention mechanism may help fuse the vision and text feataures, which can result in better feature extraction during early stages\n",
    "  - But this extra component will introduce more parameters to the network, which will require longer training to converge\n",
    "- Adding top-down attention will not help when vision and text encoders are trained with VLP, and at the same time it will slow down convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26cd2ad-af8b-4526-b53b-63d474c8c20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
